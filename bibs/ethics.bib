@article{dendral,
abstract = {The DENDRAL Project was one of the first large-scale programs to embody the strategy of using detailed, task-specific knowledge about a problem domain as a source of heuristics, and to seek generality through automating the acquisition of such knowledge. This paper summarizes the major conceptual contributions and accomplishments of that project. It is an attempt to distill from this research the lessons that are of importance to artificial intelligence research and to provide a record of the final status of two decades of work.},
author = {Lindsay, Robert K and Buchanan, Bruce G and Feigenbaum, Edward A and Lederberg, Joshua},
doi = {https://doi.org/10.1016/0004-3702(93)90068-M},
issn = {0004-3702},
journal = {Artificial Intelligence},
keywords = { DENDRAL, heuristic programming, mass spectrometry, organic chemistry, scientific discovery,Expert systems},
number = {2},
pages = {209--261},
title = {{DENDRAL: A case study of the first expert system for scientific hypothesis formation}},
url = {http://www.sciencedirect.com/science/article/pii/000437029390068M},
volume = {61},
year = {1993}
}

@article{differentialprivacy,
  author    = {Zhanglong Ji and
               Zachary Chase Lipton and
               Charles Elkan},
  title     = {Differential Privacy and Machine Learning: a Survey and Review},
  journal   = {CoRR},
  volume    = {abs/1412.7584},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.7584},
  archivePrefix = {arXiv},
  eprint    = {1412.7584},
  timestamp = {Mon, 13 Aug 2018 16:46:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/JiLE14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Obermeyer447,
abstract = {The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care.Science, this issue p. 447; see also p. 421Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5{\%}. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts.},
author = {Obermeyer, Ziad and Powers, Brian and Vogeli, Christine and Mullainathan, Sendhil},
doi = {10.1126/science.aax2342},
issn = {0036-8075},
journal = {Science},
number = {6464},
pages = {447--453},
publisher = {American Association for the Advancement of Science},
title = {{Dissecting racial bias in an algorithm used to manage the health of populations}},
url = {https://science.sciencemag.org/content/366/6464/447},
volume = {366},
year = {2019}
}

@article{burlina2020addressing,
  title={Addressing Artificial Intelligence Bias in Retinal Disease Diagnostics},
  author={Burlina, Philippe and Joshi, Neil and Paul, William and Pacheco, Katia D and Bressler, Neil M},
  journal={arXiv preprint arXiv:2004.13515},
  year={2020}
}

@Article{bourne2011,
   Author="Bourne, R. R. ",
   Title="{{E}thnicity and ocular imaging}",
   Journal="Eye (Lond)",
   Year="2011",
   Volume="25",
   Number="3",
   Pages="297--300",
   Month={5}
}

@online{ico,
    title={Anonymisation: managing data protection risk code of practice},
    organization={Information Comissioner's Office},
    url = {https://ico.org.uk/media/1061/anonymisation-code.pdf},
}

@online{gdpr,
    title = {EU General Data Protection Regulation},
    url = {https://www.privacy-regulation.eu/en/recital-26-GDPR.htm},
    organization = {European Commission},
}